{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "mount_file_id": "1isB5DP3cyRzssqrjRaWcaLCzbR28BKWf",
   "authorship_tag": "ABX9TyPGEHi2lP3lZT9d9qNpvyql"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# @title 1. Configuración\n",
    "!pip install gensim --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "# Para entrenar tus propios vectores\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Semillas\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)"
   ],
   "metadata": {
    "id": "2JX1azUvybGg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title 2. Carga y Preparación de Datos\n",
    "URL_BASE = '/kaggle/input/nlp-getting-started/'\n",
    "df = pd.read_csv(URL_BASE + \"train.csv\")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    s = str(text).lower()\n",
    "    s = s.replace(\"#\", \" \")\n",
    "    s = re.sub(r'http\\S+', '', s)\n",
    "    s = re.sub(r'[^a-z0-9\\s]', '', s)\n",
    "    return s.strip()\n",
    "\n",
    "# Limpiamos\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "df['keyword_clean'] = df['keyword'].apply(clean_text)\n",
    "\n",
    "# Concatenamos KEYWORD + TEXTO\n",
    "df['final_text'] = df['keyword_clean'].fillna('') + \" \" + df['text_clean']\n",
    "df['final_text'] = df['final_text'].str.strip()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df[\"final_text\"].values, df[\"target\"].values,\n",
    "    test_size=0.2, random_state=SEED, stratify=df[\"target\"]\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n",
    ")\n"
   ],
   "metadata": {
    "id": "0VPH1gdjyccZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title 3. Tokenización y Entrenamiento Word2Vec\n",
    "VOCAB_SIZE = 20000\n",
    "OOV_TOKEN = \"<UNK>\"\n",
    "MAX_LEN = 50\n",
    "\n",
    "# A. Tokenizer de Keras\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convertir a secuencias y padding\n",
    "def texts_to_padded(x):\n",
    "    seqs = tokenizer.texts_to_sequences(x)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "Xtr = texts_to_padded(X_train)\n",
    "Xva = texts_to_padded(X_val)\n",
    "Xte = texts_to_padded(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_len = min(VOCAB_SIZE, len(word_index) + 1)\n",
    "\n",
    "train_tokens = [t.split() for t in X_train]\n",
    "\n",
    "W2V_DIM = 100\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=train_tokens,\n",
    "    vector_size=W2V_DIM,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    seed=SEED,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_len, W2V_DIM))\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= vocab_len: continue\n",
    "    if word in w2v.wv:\n",
    "        embedding_matrix[idx] = w2v.wv[word]\n",
    "        hits += 1\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(W2V_DIM,))\n",
    "        misses += 1\n"
   ],
   "metadata": {
    "id": "uhD7g5azyfuC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title 4. Definición y Entrenamiento\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=train_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    seed=SEED,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_len, 100))\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= vocab_len: continue\n",
    "    if word in w2v.wv:\n",
    "        embedding_matrix[idx] = w2v.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(100,))\n",
    "\n",
    "modelRNN = models.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_len,\n",
    "        output_dim=100,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_LEN,\n",
    "        trainable=True,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "\n",
    "    layers.SpatialDropout1D(0.3),\n",
    "\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=False)),\n",
    "\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "modelRNN.compile(optimizer=optimizers.Adam(0.0005),\n",
    "                 loss=\"binary_crossentropy\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopper = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "hist = modelRNN.fit(Xtr, y_train,\n",
    "                    epochs=30,\n",
    "                    validation_data=(Xva, y_val),\n",
    "                    callbacks=[early_stopper],\n",
    "                    batch_size=32)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGGnsh7oyhrA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763669995646,
     "user_tz": 180,
     "elapsed": 211731,
     "user": {
      "displayName": "PUEYRREDON MANUEL",
      "userId": "14407257931475302378"
     }
    },
    "outputId": "2239a96b-a8dd-4cab-e6ab-b382f5ca356b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 135ms/step - accuracy: 0.5938 - loss: 0.6598 - val_accuracy: 0.7201 - val_loss: 0.5441\n",
      "Epoch 2/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 89ms/step - accuracy: 0.7248 - loss: 0.5526 - val_accuracy: 0.7582 - val_loss: 0.4897\n",
      "Epoch 3/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 87ms/step - accuracy: 0.7787 - loss: 0.4842 - val_accuracy: 0.7753 - val_loss: 0.4853\n",
      "Epoch 4/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 86ms/step - accuracy: 0.8191 - loss: 0.4240 - val_accuracy: 0.7832 - val_loss: 0.4750\n",
      "Epoch 5/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 90ms/step - accuracy: 0.8475 - loss: 0.3718 - val_accuracy: 0.7963 - val_loss: 0.4491\n",
      "Epoch 6/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 85ms/step - accuracy: 0.8657 - loss: 0.3319 - val_accuracy: 0.7792 - val_loss: 0.5004\n",
      "Epoch 7/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 84ms/step - accuracy: 0.8851 - loss: 0.2924 - val_accuracy: 0.7845 - val_loss: 0.5334\n",
      "Epoch 8/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 87ms/step - accuracy: 0.8961 - loss: 0.2640 - val_accuracy: 0.7766 - val_loss: 0.5672\n",
      "Epoch 9/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 92ms/step - accuracy: 0.9127 - loss: 0.2240 - val_accuracy: 0.7438 - val_loss: 0.6371\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "y_pred_prob_train = modelRNN.predict(Xtr)\n",
    "umbrales = np.arange(0.1, 0.9, 0.01)\n",
    "mejor_umbral = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for u in umbrales:\n",
    "    y_pred_t = (y_pred_prob_train >= t).astype(int)\n",
    "    f1 = f1_score(y_train, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        mejor_umbral = u\n",
    "\n",
    "\n",
    "y_pred_val = (modelRNN.predict(Xva) >= mejor_umbral).astype(int)\n",
    "\n",
    "f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "precision_1 = precision_score(y_val, y_pred_val, pos_label=1)\n",
    "precision_0 = precision_score(y_val, y_pred_val, pos_label=0)\n",
    "recall_1 = recall_score(y_val, y_pred_val, pos_label=1)\n",
    "recall_0 = recall_score(y_val, y_pred_val, pos_label=0)\n",
    "\n",
    "print(f'F1 {f1:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision_0: {precision_0:.4f}, Precision_1: {precision_1:.4f}')\n",
    "print(f'Recall_0: {recall_0:.4f}, Recall_1: {recall_1:.4f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_u_Pnnq9VqF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763671221838,
     "user_tz": 180,
     "elapsed": 3979,
     "user": {
      "displayName": "PUEYRREDON MANUEL",
      "userId": "14407257931475302378"
     }
    },
    "outputId": "57b77d23-14a9-40c1-9bff-01f6b57f64f9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "F1 0.5088\n",
      "Accuracy: 0.5611\n",
      "Precision_0: 0.9098, Precision_1: 0.4945\n",
      "Recall_0: 0.2558, Recall_1: 0.9664\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Modelo GRU ---\n",
    "modelGRU = models.Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_len,\n",
    "        output_dim=100,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_LEN,\n",
    "        trainable=True,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "\n",
    "    layers.SpatialDropout1D(0.2),  # menos dropout que antes\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=False)),\n",
    "\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "modelGRU.compile(\n",
    "    optimizer=optimizers.Adam(0.0005),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "early_stopper = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Entrenamiento ---\n",
    "hist = modelGRU.fit(\n",
    "    Xtr, y_train,\n",
    "    validation_data=(Xva, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopper]\n",
    ")\n",
    "\n",
    "y_pred_prob_train = modelGRU.predict(Xtr)\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "best_thresh = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_pred_prob_train >= t).astype(int)\n",
    "    f1 = f1_score(y_train, y_pred_t)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(\"Mejor umbral (train):\", best_thresh)\n",
    "\n",
    "# --- Evaluación en VALIDATION ---\n",
    "y_pred_val = (modelGRU.predict(Xva) >= best_thresh).astype(int)\n",
    "\n",
    "f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "precision_1 = precision_score(y_val, y_pred_val, pos_label=1)\n",
    "precision_0 = precision_score(y_val, y_pred_val, pos_label=0)\n",
    "recall_1 = recall_score(y_val, y_pred_val, pos_label=1)\n",
    "recall_0 = recall_score(y_val, y_pred_val, pos_label=0)\n",
    "\n",
    "print(f'F1: {f1:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision_0: {precision_0:.4f}, Precision_1: {precision_1:.4f}')\n",
    "print(f'Recall_0: {recall_0:.4f}, Recall_1: {recall_1:.4f}')\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred_val))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRxhkMS7_8cl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763671495151,
     "user_tz": 180,
     "elapsed": 230231,
     "user": {
      "displayName": "PUEYRREDON MANUEL",
      "userId": "14407257931475302378"
     }
    },
    "outputId": "9b4fd1f8-748f-41cd-f2ec-e01f5944907b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 149ms/step - accuracy: 0.6222 - loss: 0.6410 - val_accuracy: 0.7385 - val_loss: 0.5183\n",
      "Epoch 2/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - accuracy: 0.7570 - loss: 0.5050 - val_accuracy: 0.7819 - val_loss: 0.4721\n",
      "Epoch 3/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - accuracy: 0.8303 - loss: 0.3949 - val_accuracy: 0.7871 - val_loss: 0.5077\n",
      "Epoch 4/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 100ms/step - accuracy: 0.8955 - loss: 0.2659 - val_accuracy: 0.7582 - val_loss: 0.6364\n",
      "Epoch 5/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 110ms/step - accuracy: 0.9480 - loss: 0.1552 - val_accuracy: 0.7530 - val_loss: 0.9343\n",
      "Epoch 6/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 110ms/step - accuracy: 0.9701 - loss: 0.0926 - val_accuracy: 0.7490 - val_loss: 1.0813\n",
      "Epoch 7/30\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 110ms/step - accuracy: 0.9784 - loss: 0.0635 - val_accuracy: 0.7464 - val_loss: 1.1493\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step\n",
      "Mejor umbral (train): 0.6499999999999997\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "F1 (weighted): 0.7751\n",
      "Accuracy: 0.7845\n",
      "Precision_0: 0.7491, Precision_1: 0.8721\n",
      "Recall_0: 0.9355, Recall_1: 0.5841\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.94      0.83       434\n",
      "           1       0.87      0.58      0.70       327\n",
      "\n",
      "    accuracy                           0.78       761\n",
      "   macro avg       0.81      0.76      0.77       761\n",
      "weighted avg       0.80      0.78      0.78       761\n",
      "\n"
     ]
    }
   ]
  }
 ]
}